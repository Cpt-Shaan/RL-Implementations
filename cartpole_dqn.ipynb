{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+Zb0uNtg4YaQv908qouck",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cpt-Shaan/RL-Implementations/blob/main/cartpole_dqn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1j41rA4geesW"
      },
      "outputs": [],
      "source": [
        "# Importing required libraries\n",
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from collections import namedtuple,deque\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Q-Network structure\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size, seed, fc1_nodes = 64, fc2_nodes = 64):\n",
        "        super(DQN, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, fc1_nodes)\n",
        "        self.fc2 = nn.Linear(fc1_nodes, fc2_nodes)\n",
        "        self.fc3 = nn.Linear(fc2_nodes, action_size)\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        ""
      ],
      "metadata": {
        "id": "WOxX_3ycXOVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replay memory class\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen = buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k = self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).float().to(device)"
      ],
      "metadata": {
        "id": "AiD1WugCXP4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class for the DQN-Agent\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, seed, lr):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.localNet = DQN(state_size, action_size, seed).to(device)\n",
        "        self.targetNet = DQN(state_size, action_size, seed).to(device)\n",
        "        self.optimizer = optim.Adam(self.localNet.parameters(), lr)\n",
        "        self.memory = ReplayMemory(action_size, buffer_size = int(1e5), batch_size = 64, seed = seed)\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        self.t_step = (self.t_step + 1) % 4\n",
        "        if self.t_step == 0:\n",
        "            if(len(self.memory) > 64):\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, gamma = 0.99)\n",
        "\n",
        "    # Chosse an action based on the epsilon-greedy poilcy\n",
        "    def act(self, state, eps = 0.):\n",
        "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "\n",
        "        self.localNet.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.localNet(state_tensor)\n",
        "\n",
        "        self.localNet.train()\n",
        "\n",
        "        if(np.random.random() > eps):\n",
        "            return action_values.argmax(dim = 1).item()\n",
        "        else:\n",
        "            return np.random.randint(self.action_size)\n",
        "\n",
        "    # Learn from the experiences in the replay-memory\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "        states = torch.from_numpy(np.vstack(states)).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack(actions)).long().to(device)  # Use long for gather\n",
        "        rewards = torch.from_numpy(np.vstack(rewards)).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack(next_states)).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack(dones)).float().to(device)\n",
        "\n",
        "        next_targets = self.targetNet(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        targets = rewards + (gamma * next_targets * (1 - dones))\n",
        "        expected_qvalue = self.localNet(states).gather(1, actions)\n",
        "\n",
        "        loss = F.mse_loss(expected_qvalue, targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.soft_update(self.localNet, self.targetNet, tau = 1e-3)\n",
        "\n",
        "\n",
        "    def soft_update(self, localNet, targetNet, tau):\n",
        "        for target_param, local_param in zip(targetNet.parameters(), localNet.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n"
      ],
      "metadata": {
        "id": "ccCUjEjpXR-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Hyperparameters\n",
        "num_episodes = 3000\n",
        "max_steps = 200\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.2\n",
        "epsilon_decay_rate = 0.99\n",
        "gamma = 0.9\n",
        "lr = 0.0025\n",
        "buffer_size = 10000\n",
        "buffer = deque(maxlen=buffer_size)\n",
        "batch_size = 128\n",
        "\n",
        "# Initialize DQN-Agent\n",
        "input_dim = env.observation_space.shape[0]\n",
        "output_dim = env.action_space.n\n",
        "new_agent = DQNAgent(input_dim, output_dim, seed=69691, lr = lr)"
      ],
      "metadata": {
        "id": "mp6qTWrrXThX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_rewards = 0\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()[0]\n",
        "    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay_rate ** episode))\n",
        "\n",
        "    # One episode\n",
        "    for step in range(max_steps):\n",
        "        action = new_agent.act(state,epsilon)\n",
        "        next_state,reward,done,_,_ = env.step(action)\n",
        "        total_rewards += reward\n",
        "        buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "        if(len(buffer) >= batch_size):\n",
        "            batch = random.sample(buffer, batch_size)\n",
        "\n",
        "            # update the agent's knowledge\n",
        "            new_agent.learn(batch, gamma)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    if (episode + 1) % 20 == 0:\n",
        "        avg_reward = total_rewards / 20\n",
        "        print(f\"Episode {episode + 1} : Finished Training, Average Rewards over last 20 episodes : {avg_reward:0.2f}\")\n",
        "        total_rewards = 0"
      ],
      "metadata": {
        "id": "dfbZYx6rXVJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the agent's performance\n",
        "test_episodes = 100\n",
        "episode_rewards = []\n",
        "\n",
        "for episode in range(test_episodes):\n",
        "    state = env.reset()[0]\n",
        "    episode_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = new_agent.act(state, eps=0.)\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "\n",
        "average_reward = sum(episode_rewards) / test_episodes\n",
        "print(f\"Average reward over {test_episodes} test episodes: {average_reward:.2f}\")"
      ],
      "metadata": {
        "id": "wrvKTJNAXXOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the agent's performance\n",
        "import time\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
        "state = env.reset()[0]\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "\n",
        "    action = new_agent.act(state, eps=0.)\n",
        "    next_state, reward, done, _ , _ = env.step(action)\n",
        "    state = next_state\n",
        "    time.sleep(0.1)  # Add a delay to make the visualization easier to follow\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "ofbCyNeZXZE7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}